{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression Toolkit - Experiments\n",
        "\n",
        "This notebook demonstrates the Linear Regression toolkit with three experiments:\n",
        "1. Straight Line with Noise\n",
        "2. Collinearity and Ridge Regularization\n",
        "3. Polynomial Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from linear_regression.linear_models import LinearRegressionClosedForm\n",
        "from linear_regression.metrics import mse, r2_score\n",
        "from linear_regression.selection import train_test_split\n",
        "from linear_regression.plotting import plot_predictions, plot_residuals\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Straight Line with Noise\n",
        "\n",
        "Generate synthetic data: $y = 3 + 2x + \\epsilon$, where $\\epsilon \\sim N(0, 1)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data: y = 3 + 2*x + noise\n",
        "n_samples = 100\n",
        "X = np.linspace(0, 10, n_samples).reshape(-1, 1)\n",
        "y_true = 3 + 2 * X.ravel()\n",
        "y = y_true + np.random.normal(0, 1, n_samples)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit model with alpha=0.0 (no regularization)\n",
        "model = LinearRegressionClosedForm(fit_intercept=True, alpha=0.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Report results\n",
        "print(\"Learned coefficients:\")\n",
        "print(f\"  Intercept: {model.intercept_:.4f} (true: 3.0)\")\n",
        "print(f\"  Coefficient: {model.coef_[0]:.4f} (true: 2.0)\")\n",
        "print(f\"\\nTrain MSE: {mse(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Test MSE: {mse(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Train R²: {r2_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Test R²: {r2_score(y_test, y_test_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predicted vs true values\n",
        "plot_predictions(y_test, y_test_pred, title=\"Experiment 1: Predictions vs True Values\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: Collinearity and Ridge Regularization\n",
        "\n",
        "Create two highly correlated features and fit models with different alpha values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create collinear features: x2 = x1 + 0.01 * noise\n",
        "n_samples = 200\n",
        "x1 = np.random.randn(n_samples)\n",
        "x2 = x1 + 0.01 * np.random.randn(n_samples)\n",
        "X = np.column_stack([x1, x2])\n",
        "\n",
        "# True relationship: y = x1 + x2 + noise\n",
        "y = x1 + x2 + 0.1 * np.random.randn(n_samples)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit models with different alpha values\n",
        "alphas = [0.0, 1e-3, 1e-2, 1e-1, 1.0]\n",
        "results = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    model = LinearRegressionClosedForm(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_mse = mse(y_test, y_test_pred)\n",
        "    coef_norm = np.linalg.norm(model.coef_)\n",
        "    results.append({\n",
        "        'alpha': alpha,\n",
        "        'test_mse': test_mse,\n",
        "        'coef_norm': coef_norm,\n",
        "        'coef': model.coef_.copy()\n",
        "    })\n",
        "    print(f\"Alpha={alpha:6.4f}: Test MSE={test_mse:.4f}, ||coef||₂={coef_norm:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "alphas_plot = [r['alpha'] for r in results]\n",
        "mse_plot = [r['test_mse'] for r in results]\n",
        "norm_plot = [r['coef_norm'] for r in results]\n",
        "\n",
        "ax1.semilogx(alphas_plot, mse_plot, 'o-')\n",
        "ax1.set_xlabel('Alpha (regularization strength)')\n",
        "ax1.set_ylabel('Test MSE')\n",
        "ax1.set_title('Test MSE vs Alpha')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.semilogx(alphas_plot, norm_plot, 'o-')\n",
        "ax2.set_xlabel('Alpha (regularization strength)')\n",
        "ax2.set_ylabel('||coef||₂')\n",
        "ax2.set_title('Coefficient Norm vs Alpha')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion\n",
        "\n",
        "As alpha increases, the L2 regularization penalty becomes stronger. This causes:\n",
        "1. **Coefficient shrinkage**: The weight vector norm ||coef||₂ decreases, making the model simpler and less prone to overfitting.\n",
        "2. **Test MSE behavior**: Initially, regularization may improve generalization by reducing overfitting. However, if alpha becomes too large, the model becomes underfitted and test MSE increases.\n",
        "3. **Stability**: With collinear features, regularization helps stabilize the solution by preventing coefficients from becoming extremely large.\n",
        "\n",
        "In this experiment, we can see that moderate regularization (alpha around 0.01-0.1) helps control the coefficient magnitudes while maintaining good predictive performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: Polynomial Regression\n",
        "\n",
        "Generate nonlinear data: $y = 1 + 2x - 0.3x^2 + \\epsilon$. Compare degree 1, 2, and 5 polynomial features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate nonlinear data: y = 1 + 2*x - 0.3*x^2 + noise\n",
        "n_samples = 100\n",
        "X = np.linspace(-5, 5, n_samples).reshape(-1, 1)\n",
        "y_true = 1 + 2 * X.ravel() - 0.3 * X.ravel() ** 2\n",
        "y = y_true + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Helper function to create polynomial features\n",
        "def polynomial_features(X, degree):\n",
        "    \"\"\"Create polynomial features up to given degree.\"\"\"\n",
        "    features = [X ** d for d in range(1, degree + 1)]\n",
        "    return np.column_stack(features)\n",
        "\n",
        "# Compare different polynomial degrees\n",
        "degrees = [1, 2, 5]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, degree in enumerate(degrees):\n",
        "    # Create polynomial features\n",
        "    X_train_poly = polynomial_features(X_train, degree)\n",
        "    X_test_poly = polynomial_features(X_test, degree)\n",
        "    \n",
        "    # Fit model\n",
        "    model = LinearRegressionClosedForm(alpha=0.0)\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Predictions for plotting\n",
        "    X_plot = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
        "    X_plot_poly = polynomial_features(X_plot, degree)\n",
        "    y_plot_pred = model.predict(X_plot_poly)\n",
        "    \n",
        "    # Test predictions\n",
        "    y_test_pred = model.predict(X_test_poly)\n",
        "    test_mse = mse(y_test, y_test_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    \n",
        "    # Plot\n",
        "    ax = axes[idx]\n",
        "    ax.scatter(X_train, y_train, alpha=0.5, label='Train', s=20)\n",
        "    ax.scatter(X_test, y_test, alpha=0.5, label='Test', s=20, marker='^')\n",
        "    ax.plot(X_plot, y_true, 'g--', label='True function', linewidth=2)\n",
        "    ax.plot(X_plot, y_plot_pred, 'r-', label='Fitted', linewidth=2)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'Degree {degree}\\nTest MSE: {test_mse:.4f}, R²: {test_r2:.4f}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comment\n",
        "\n",
        "**Which degree fits best and why?**\n",
        "\n",
        "- **Degree 1 (Linear)**: Underfits the data. The true function is quadratic, so a linear model cannot capture the curvature. Test MSE is high.\n",
        "\n",
        "- **Degree 2 (Quadratic)**: Fits best! This matches the true underlying function (y = 1 + 2x - 0.3x²). The model has just enough flexibility to capture the quadratic relationship without overfitting. Test MSE is lowest and R² is highest.\n",
        "\n",
        "- **Degree 5 (High-order polynomial)**: Overfits the data. While it can fit the training data very well, it captures noise and generalizes poorly to test data. The high-order terms cause the curve to wiggle unnecessarily, leading to higher test MSE.\n",
        "\n",
        "**Conclusion**: Degree 2 is optimal because it matches the true model complexity. Higher degrees overfit, while lower degrees underfit.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
